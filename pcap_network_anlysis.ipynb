{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pcap network anlysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqZLQra5mDNH3CcA6HctFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paradoxxs/Paradoxxs.github.io/blob/main/pcap_network_anlysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using NFStream to convert the pcap file to dataframe, and there after perform analysis."
      ],
      "metadata": {
        "id": "FO1kkYZtCloZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nfstream"
      ],
      "metadata": {
        "id": "GcC3cKgEb28N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9aNCJ0fV0ZW"
      },
      "outputs": [],
      "source": [
        "import nfstream\n",
        "from nfstream import NFStreamer, NFPlugin\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = NFStreamer(source=\"file.pcap\").to_pandas()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Ed-JT7ZcV_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = 'bidirectional_first_seen_ms'\n",
        "src_ip = 'src_ip'\n",
        "dst_ip = 'dst_ip'\n",
        "dst_host = 'requested_server_name'\n",
        "dst_port = 'dst_port'\n",
        "bytes_sent = 'src2dst_bytes'\n",
        "\n",
        "filter = [timestamp, src_ip, dst_ip, dst_host, dst_port, bytes_sent]\n",
        "groupby = [src_ip, dst_ip, dst_port] #Group the connect together that are the same. \n"
      ],
      "metadata": {
        "id": "ATkcY3JO3f1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[:,filter]\n",
        "df[timestamp] = pd.to_datetime(df[timestamp], unit='ms')  #Converting ms to datetime\n",
        "df = df.groupby(groupby).agg(list)\n",
        "df.reset_index(inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zXSjYy1N3iBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkvAJPrBBKuu",
        "outputId": "35578b9f-bdbd-4acc-b17b-7fdc30b3327f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ConnectionCount is  by taking each row in the timestamp column, and get the about of connection that have been made\n",
        "df['ConnectionCount'] = df[timestamp].apply(lambda x: len(x))\n",
        "\n",
        "#Remove all connection with less then 10 connections, it was choosen because of the small data sample I used, The goal is to reduce the amount of data that need to be processed\n",
        "df = df.loc[df['ConnectionCount'] > 10]\n",
        "\n",
        "#Sort the data\n",
        "df[timestamp] = df[timestamp].apply(lambda x: sorted(x))\n",
        "\n",
        "\n",
        "df['delta_time'] = df[timestamp].apply(lambda x: pd.Series(x).diff().dt.seconds.dropna().tolist())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Eq4z7lMV3ppD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tsLow'] = df['delta_time'].apply(lambda x: np.percentile(np.array(x),25))\n",
        "df['tsMid'] = df['delta_time'].apply(lambda x: np.percentile(np.array(x), 50))\n",
        "df['tsHigh'] = df['delta_time'].apply(lambda x: np.percentile(np.array(x), 75))\n",
        "\n",
        "df['tsBowleyNum'] = df['tsLow'] + df['tsHigh'] - 2 * df['tsMid']\n",
        "df['tsBowleyDen'] = df['tsHigh'] - df['tsLow']\n",
        "\n",
        "# tsSkew should equal zero if the denominator equals zero\n",
        "# bowley skew is unreliable if Q2 = Q1 or Q2 = Q3\n",
        "df['tsSkew'] = df[['tsLow', 'tsMid', 'tsHigh', 'tsBowleyNum','tsBowleyDen']].apply(\n",
        "    lambda x: x['tsBowleyNum'] / x['tsBowleyDen'] if x['tsBowleyDen'] !=0 and x['tsMid'] != x['tsLow'] and x['tsMid'] != x['tsHigh'] !=0 else 0.0, axis=1\n",
        "    )\n",
        "df['tsMadm'] = df['delta_time'].apply(lambda x: np.median(np.absolute(np.array(x) - np.median(np.array(x)))))\n",
        "df['tsConnDiv'] = df[f_timestamp].apply(lambda x: (x[-1].to_pydatetime() - x[0].to_pydatetime()).seconds / 90)\n",
        "\n",
        "# Time delta score calculation\n",
        "df['tsConnCountScore'] = df.apply(lambda x: 0.0 if x['tsConnDiv'] == 0  else x['ConnectionCount'] / x['tsConnDiv'] if x['ConnectionCount'] / x['tsConnDiv'] < 1.0 else 1.0 , axis=1)\n",
        "df['tsSkewScore'] = 1.0 - abs(df['tsSkew'])\n",
        "df['tsMadmScore'] = df['tsMadm'].apply(lambda x: 0 if 1.0 - (x / 30.0) < 0 else 1.0 - (x / 30.0))\n",
        "df['tsScore'] = (((df['tsSkewScore'] + df['tsMadmScore'] + df['tsConnCountScore']) / 3.0) * 1000) / 1000\n"
      ],
      "metadata": {
        "id": "KFowLjsX3vRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['dsLow'] = df[f_sent_bytes].apply(lambda x: np.percentile(np.array(x), 25))\n",
        "df['dsMid'] = df[f_sent_bytes].apply(lambda x: np.percentile(np.array(x), 50))\n",
        "df['dsHigh'] = df[f_sent_bytes].apply(lambda x: np.percentile(np.array(x), 75))\n",
        "df['dsBowleyNum'] = df['dsLow'] + df['dsHigh'] - 2 * df['dsMid']\n",
        "df['dsBowleyDen'] = df['dsHigh'] - df['dsLow']\n",
        "\n",
        "\n",
        "# dsSkew should equal zero if the denominator equals zero\n",
        "# bowley skew is unreliable if Q2 = Q1 or Q2 = Q3\n",
        "df['dsSkew'] = df[['dsLow','dsMid','dsHigh','dsBowleyNum','dsBowleyDen']].apply(\n",
        "    lambda x: x['dsBowleyNum'] / x['dsBowleyDen'] if x['dsBowleyDen'] != 0 and x['dsMid'] != x['dsLow'] and x['dsMid'] != x['dsHigh'] else 0.0, axis=1\n",
        "    )\n",
        "df['dsMadm'] = df[f_sent_bytes].apply(lambda x: np.median(np.absolute(np.array(x) - np.median(np.array(x)))))\n",
        "\n",
        "\n",
        "# Data size score calculation of sent bytes\n",
        "df['dsSkewScore'] = 1.0 - abs(df['dsSkew'])\n",
        "df['dsMadmScore'] = df['dsMadm'].apply(lambda x: 0 if x/ 128.0 < 0 else x/ 128.0)\n",
        "df['dsSmallnessScore'] = df['dsMid'].apply(lambda x: 0 if 1.0 - x / 8192.0 < 0 else 1.0 - x / 8192.0)\n",
        "df['dsScore'] = (((df['dsSkewScore'] + df['dsMadmScore'] + df['dsSmallnessScore']) / 3.0) * 1000) / 1000"
      ],
      "metadata": {
        "id": "HJH8AJGk3wsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overal Score calculation\n",
        "df['Score'] = (df['dsScore'] + df['tsScore']) / 2\n",
        "\n",
        "df = df.sort_values(by= 'Score')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3i-btTlR3x5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}