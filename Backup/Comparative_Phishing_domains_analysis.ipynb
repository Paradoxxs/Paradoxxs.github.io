{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparative Phishing domains analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP57ce7aBp/vuvNF/0KmH4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paradoxxs/Paradoxxs.github.io/blob/main/Comparative_Phishing_domains_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JYZlX-fHCMb"
      },
      "source": [
        "The usecase of this project is to detect phishing domains in log files. \n",
        "This is done by using a random forest classification tree to detect domain generated algorihms and levensthtein to detect domain that similar in nature to the top 1m domains. \n",
        "\n",
        "Compared to the simple analysis this once compares the different algorihms and select the best one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGW2OZPFakDF"
      },
      "source": [
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import ensemble\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import tree, linear_model\n",
        "import sklearn.ensemble as ske\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import Counter"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he_yrWuFKOv4"
      },
      "source": [
        "def entropy(s):\n",
        "    p, lns = Counter(s), float(len(s))\n",
        "    return -sum(count / lns * math.log(count / lns, 2) for count in list(p.values()))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWgRE56N7pm8"
      },
      "source": [
        "Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNctSWgl7o_y"
      },
      "source": [
        "dga_df = pd.read_csv('https://raw.githubusercontent.com/austin-taylor/flare/master/flare/data/misc/dga_domains.txt', names=['raw_domain'], header=None, encoding='utf-8')\n",
        "word_df = pd.read_csv('https://raw.githubusercontent.com/austin-taylor/flare/master/flare/data/misc/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')\n",
        "alexa_df = pd.read_csv('https://raw.githubusercontent.com/austin-taylor/flare/master/flare/data/alexa/top-1m.csv')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yma91ag6HAH9"
      },
      "source": [
        "Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nND60BeD7ttP",
        "outputId": "897373c6-2b17-49db-8343-1c1b74ea7935"
      },
      "source": [
        "alexa_df.columns = ['uri']\n",
        "alexa_df['domain'] = 'empt'\n",
        "alexa_df['domain'] =  [alexa_df['uri'][idx].split('.')[0].strip().lower() for idx in alexa_df.index ]\n",
        "alexa_df = alexa_df.dropna()\n",
        "alexa_df = alexa_df.drop_duplicates()\n",
        "alexa_df['class'] = 'legit'\n",
        "alexa_df = alexa_df.reindex(np.random.permutation(alexa_df.index))\n",
        "alexa_total = alexa_df.shape[0]\n",
        "alexa_df = alexa_df[:int(alexa_total * .9)]\n",
        "alexa_df[:10].index"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([147592, 5148, 723318, 524407, 749662, 273432, 235677, 362188,\n",
              "            823896, 801630],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6VO5CmM7y0A"
      },
      "source": [
        "# Remove Top level domain\n",
        "dga_df['domain'] = 'empt'\n",
        "dga_df['domain'] =  [dga_df['raw_domain'][idx].split('.')[0].strip().lower() for idx in dga_df.index]\n",
        "\n",
        "# Cleanup any blank lines or dups\n",
        "dga_df = dga_df.dropna()\n",
        "dga_df = dga_df.drop_duplicates()\n",
        "dga_total = dga_df.shape[0]\n",
        "\n",
        "# Set Class\n",
        "dga_df['class'] = 'dga'\n",
        "\n",
        "# Hold out 10% of DGA\n",
        "dga_df = dga_df[:int(dga_total * .9)]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXHVdGjH8AMx"
      },
      "source": [
        "# Read in word dictionary for trigrams\n",
        "word_df = word_df[word_df['word'].map(lambda x: str(x).isalpha())]\n",
        "word_df = word_df.applymap(lambda x: str(x).strip().lower())\n",
        "word_df = word_df.dropna()\n",
        "word_df = word_df.drop_duplicates()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0y8N44-76hr"
      },
      "source": [
        "# Merge Domains\n",
        "all_domains = pd.concat([alexa_df, dga_df], ignore_index=True)\n",
        "\n",
        "# Features\n",
        "all_domains['length'] = [len(x) for x in all_domains['domain']]\n",
        "all_domains = all_domains[all_domains['length'] > 6]\n",
        "all_domains['entropy'] = [entropy(\n",
        "    x) for x in all_domains['domain']]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uA4MImo8f_v",
        "outputId": "709e9f05-5640-4471-b27a-32b2382949a2"
      },
      "source": [
        "alexa_vc = feature_extraction.text.CountVectorizer(\n",
        "    analyzer='char', ngram_range=(3, 5), min_df=1e-4, max_df=1.0)\n",
        "print(alexa_vc)\n",
        "counts_matrix = alexa_vc.fit_transform(alexa_df['domain'])\n",
        "alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
            "                ngram_range=(3, 5), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lls-SxQ2bw5S"
      },
      "source": [
        "dict_vc = feature_extraction.text.CountVectorizer(\n",
        "    analyzer='char', ngram_range=(3, 5), min_df=1e-5, max_df=1.0)\n",
        "counts_matrix = dict_vc.fit_transform(word_df['word'])\n",
        "dict_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
        "\n",
        "all_domains['alexa_grams'] = alexa_counts * \\\n",
        "    alexa_vc.transform(all_domains['domain']).T\n",
        "all_domains['word_grams'] = dict_counts * \\\n",
        "    dict_vc.transform(all_domains['domain']).T\n",
        "all_domains['diff'] = all_domains[\n",
        "    'alexa_grams'] - all_domains['word_grams']\n",
        "\n",
        "weird_cond = (all_domains['class'] == 'legit') & (\n",
        "    all_domains['word_grams'] < 3) & (all_domains['alexa_grams'] < 2)\n",
        "\n",
        "\n",
        "not_weird = all_domains[all_domains['class'] != 'weird']\n",
        "X = not_weird[['length', 'entropy', 'alexa_grams', 'word_grams']].to_numpy()\n",
        "\n",
        "# Labels (scikit learn uses 'y' for classification labels)\n",
        "y = np.array(not_weird['class'].tolist())"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhiHEod28TXk"
      },
      "source": [
        "# Train on a 80/20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGe3BNZU9tQ6"
      },
      "source": [
        "algorithms = {\n",
        "        \"DecisionTree\": tree.DecisionTreeClassifier(max_depth=10),\n",
        "        \"RandomForest\": ske.RandomForestClassifier(n_estimators=50),\n",
        "        \"GradientBoosting\": ske.GradientBoostingClassifier(n_estimators=50),\n",
        "        \"AdaBoost\": ske.AdaBoostClassifier(n_estimators=100),\n",
        "    }"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkYZUpPa_Z9i"
      },
      "source": [
        "Select the best argorithm and makes a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BYEv0cG9yJA",
        "outputId": "2434b509-b095-431d-8b05-8ead64d0ba16"
      },
      "source": [
        "results = {}\n",
        "print(\"\\nNow testing algorithms\")\n",
        "results = {k:v.fit(X_train, y_train).score(X_test, y_test) for (k,v) in algorithms.items()}\n",
        "print(results)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Now testing algorithms\n",
            "{'DecisionTree': 0.9978991450976585, 'RandomForest': 0.9978714110395418, 'GradientBoosting': 0.9971711260720947, 'AdaBoost': 0.9975178017985536}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpw5QJwhRUeX"
      },
      "source": [
        "Display the confusion matrix for each algorithms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1R2xXP9OsU1",
        "outputId": "69de731c-c38e-445e-f9b3-0e0b3b2775df"
      },
      "source": [
        "predictions = {}\n",
        "predictions = {k:v.predict(X_test) for (k,v) in algorithms.items()}\n",
        "results2 = {}\n",
        "for (k,v) in algorithms.items():\n",
        "  print(k)\n",
        "  mt = confusion_matrix(y_test, predictions[k])\n",
        "  print(mt)\n",
        "  print(\"False positive rate %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\n",
        "  print('False negative rate %f %%' % ((mt[1][0] / float(sum(mt[1]))*100)))\n",
        "  print('\\n')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree\n",
            "[[   220    240]\n",
            " [    63 143704]]\n",
            "False positive rate 52.173913 %\n",
            "False negative rate 0.043821 %\n",
            "\n",
            "\n",
            "RandomForest\n",
            "[[   236    224]\n",
            " [    83 143684]]\n",
            "False positive rate 48.695652 %\n",
            "False negative rate 0.057732 %\n",
            "\n",
            "\n",
            "GradientBoosting\n",
            "[[    95    365]\n",
            " [    43 143724]]\n",
            "False positive rate 79.347826 %\n",
            "False negative rate 0.029910 %\n",
            "\n",
            "\n",
            "AdaBoost\n",
            "[[   197    263]\n",
            " [    95 143672]]\n",
            "False positive rate 57.173913 %\n",
            "False negative rate 0.066079 %\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tQ44-3c_VQN",
        "outputId": "224e20f9-45be-47f7-d384-538054d90dab"
      },
      "source": [
        "winner = max(results, key=results.get)\n",
        "print('best algo %s ' % (winner))\n",
        "clf = algorithms[winner]\n",
        "res = clf.predict(X_test)\n",
        "mt = confusion_matrix(y_test, res)\n",
        "print(mt)\n",
        "print(\"False positive rate %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\n",
        "print('False negative rate %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best algo GradientBoosting \n",
            "[[    95    365]\n",
            " [    43 143724]]\n",
            "False positive rate 79.347826 %\n",
            "False negative rate 0.029910 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTt46LWa9V8l"
      },
      "source": [
        "Train the model on all the data in x and y. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN0Apzg_63Ou"
      },
      "source": [
        "    def predict(domain):\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            _alexa_match = alexa_counts * \\\n",
        "                alexa_vc.transform(\n",
        "                    [domain]).T  # Matrix multiply and transpose\n",
        "            _dict_match = dict_counts * dict_vc.transform([domain]).T\n",
        "            _X = [len(domain), entropy(\n",
        "                domain), _alexa_match, _dict_match]\n",
        "            if int(sklearn.__version__.split('.')[1]) > 20:\n",
        "                _X = [_X]\n",
        "            return clf.predict(_X)[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rej3dwTPGZEA"
      },
      "source": [
        "levenshtein is used to detect the minimum distance between two words\n",
        "https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RtXPMLW6Nhh"
      },
      "source": [
        "def levenshtein(source, target):\n",
        "    if len(source) < len(target):\n",
        "        return levenshtein(target, source)\n",
        "\n",
        "    # So now we have len(source) >= len(target).\n",
        "    if len(target) == 0:\n",
        "        return len(source)\n",
        "\n",
        "    # We call tuple() to force strings to be used as sequences\n",
        "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
        "    source = np.array(tuple(source))\n",
        "    target = np.array(tuple(target))\n",
        "\n",
        "    # We use a dynamic programming algorithm, but with the\n",
        "    # added optimization that we only need the last two rows\n",
        "    # of the matrix.\n",
        "    previous_row = np.arange(target.size + 1)\n",
        "    for s in source:\n",
        "        # Insertion (target grows longer than source):\n",
        "        current_row = previous_row + 1\n",
        "\n",
        "        # Substitution or matching:\n",
        "        # Target and source items are aligned, and either\n",
        "        # are different (cost of 1), or are the same (cost of 0).\n",
        "        current_row[1:] = np.minimum(\n",
        "            current_row[1:],\n",
        "            np.add(previous_row[:-1], target != s))\n",
        "\n",
        "        # Deletion (target grows shorter than source):\n",
        "        current_row[1:] = np.minimum(\n",
        "            current_row[1:],\n",
        "            current_row[0:-1] + 1)\n",
        "\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZzSLOUv9Ryp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5d5c02-5dde-44dd-ba8a-d3519a235e7c"
      },
      "source": [
        "#Test scripts\n",
        "print(dga_c.predict('gDKojad0'))\n",
        "levenshtein('google.com', 'gool.com')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dga\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    }
  ]
}