{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phishing domains analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEZ49LpoIH3ZaM1IN16DHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paradoxxs/Paradoxxs.github.io/blob/main/Phishing_domains_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDB5Cnh7PSK0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGW2OZPFakDF"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import ensemble\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import lightgbm as lgb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CDw0swKd5GH"
      },
      "source": [
        "class Alexa(object):\n",
        "\n",
        "    def __init__(self, limit=1000000):\n",
        "        self.limit = limit\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        return self.domain_in_alexa(word)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he_yrWuFKOv4"
      },
      "source": [
        "def entropy(s):\n",
        "    p, lns = Counter(s), float(len(s))\n",
        "    return -sum(count / lns * math.log(count / lns, 2) for count in list(p.values()))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lls-SxQ2bw5S"
      },
      "source": [
        "class dga_classifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        print('[*] Initializing... training classifier - Please wait.')\n",
        "        #self.a = Alexa()\n",
        "        dga_df = pd.read_csv('dga_domains.txt', names=['raw_domain'], header=None, encoding='utf-8')\n",
        "        word_df = pd.read_csv('words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')\n",
        "        self.entropy = entropy\n",
        "        #self.domain_extract = domain_extract\n",
        "\n",
        "        alexa_df = pd.read_csv('top-1m.csv')\n",
        "        alexa_df.columns = ['uri']\n",
        "\n",
        "        alexa_df['domain'] = 'empt'\n",
        "        alexa_df['domain'] =  [alexa_df['uri'][idx].split('.')[0].strip().lower() for idx in alexa_df.index ]\n",
        "\n",
        "        #alexa_df['domain'] = [self.domain_extract(uri) for uri in alexa_df['uri']]\n",
        "        alexa_df = alexa_df.dropna()\n",
        "        alexa_df = alexa_df.drop_duplicates()\n",
        "        alexa_df['class'] = 'legit'\n",
        "        alexa_df = alexa_df.reindex(np.random.permutation(alexa_df.index))\n",
        "        alexa_total = alexa_df.shape[0]\n",
        "        alexa_df = alexa_df[:int(alexa_total * .9)]\n",
        "        alexa_df[:10].index\n",
        "\n",
        "\n",
        "\n",
        "        # Blacklist values just differ by captilization or .com/.org/.info\n",
        "   \n",
        "\n",
        "        # Remove Top level domain\n",
        "        dga_df['domain'] = 'empt'\n",
        "        dga_df['domain'] =  [dga_df['raw_domain'][idx].split('.')[0].strip().lower() for idx in dga_df.index]\n",
        "       \n",
        "        #for idx in dga_df.index:\n",
        "         # dga_df['domain'][idx] = dga_df['raw_domain'][idx].split('.')[0].strip().lower() \n",
        "        #dga_df['domain'] = dga_df.applymap(lambda x: x.split('.')[0].strip().lower())\n",
        "      \n",
        "        # Cleanup any blank lines or dups\n",
        "        dga_df = dga_df.dropna()\n",
        "        dga_df = dga_df.drop_duplicates()\n",
        "        dga_total = dga_df.shape[0]\n",
        "\n",
        "        # Set Class\n",
        "        dga_df['class'] = 'dga'\n",
        "\n",
        "        # Hold out 10% of DGA\n",
        "        dga_df = dga_df[:int(dga_total * .9)]\n",
        "\n",
        "        # Merge Domains\n",
        "        all_domains = pd.concat([alexa_df, dga_df], ignore_index=True)\n",
        "\n",
        "        # Features\n",
        "        all_domains['length'] = [len(x) for x in all_domains['domain']]\n",
        "        all_domains = all_domains[all_domains['length'] > 6]\n",
        "        all_domains['entropy'] = [self.entropy(\n",
        "            x) for x in all_domains['domain']]\n",
        "\n",
        "\n",
        "        self.alexa_vc = feature_extraction.text.CountVectorizer(\n",
        "            analyzer='char', ngram_range=(3, 5), min_df=1e-4, max_df=1.0)\n",
        "        print(self.alexa_vc)\n",
        "        counts_matrix = self.alexa_vc.fit_transform(alexa_df['domain'])\n",
        "        self.alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
        "\n",
        "        # Read in word dictionary for trigrams\n",
        "        word_df = word_df[word_df['word'].map(lambda x: str(x).isalpha())]\n",
        "        word_df = word_df.applymap(lambda x: str(x).strip().lower())\n",
        "        word_df = word_df.dropna()\n",
        "        word_df = word_df.drop_duplicates()\n",
        "\n",
        "        self.dict_vc = feature_extraction.text.CountVectorizer(\n",
        "            analyzer='char', ngram_range=(3, 5), min_df=1e-5, max_df=1.0)\n",
        "        counts_matrix = self.dict_vc.fit_transform(word_df['word'])\n",
        "        self.dict_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
        "\n",
        "        all_domains['alexa_grams'] = self.alexa_counts * \\\n",
        "            self.alexa_vc.transform(all_domains['domain']).T\n",
        "        all_domains['word_grams'] = self.dict_counts * \\\n",
        "            self.dict_vc.transform(all_domains['domain']).T\n",
        "        all_domains['diff'] = all_domains[\n",
        "            'alexa_grams'] - all_domains['word_grams']\n",
        "\n",
        "        weird_cond = (all_domains['class'] == 'legit') & (\n",
        "            all_domains['word_grams'] < 3) & (all_domains['alexa_grams'] < 2)\n",
        "\n",
        "\n",
        "        not_weird = all_domains[all_domains['class'] != 'weird']\n",
        "        X = not_weird[['length', 'entropy', 'alexa_grams', 'word_grams']].to_numpy()\n",
        "\n",
        "        # Labels (scikit learn uses 'y' for classification labels)\n",
        "        y = np.array(not_weird['class'].tolist())\n",
        "\n",
        "        # Train on a 80/20 split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2)\n",
        "\n",
        "        print('[+] Training classifier on training set')\n",
        "\n",
        "        clf = ensemble.RandomForestClassifier(n_estimators=20, oob_score=True)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        print('[+] Out of sample legit f1 score {}'.format(f1_score(\n",
        "            y_test == 'legit', \n",
        "            y_pred == 'legit',\n",
        "            pos_label = 1,\n",
        "        )))\n",
        "\n",
        "        print('[+] Out of sample dga f1 score {}'.format(f1_score(\n",
        "            y_test == 'legit', \n",
        "            y_pred == 'legit',\n",
        "            pos_label = 0,\n",
        "        )))\n",
        "        \n",
        "        print('[+] Training final classifier')\n",
        "        self.clf = ensemble.RandomForestClassifier(n_estimators=20, oob_score=True)\n",
        "        self.clf.fit(X, y)\n",
        "        print('[+] Classifier Ready')\n",
        "\n",
        "    def ngram_count(self, domain):\n",
        "        # Multiply and transpose vector\n",
        "        alexa_match = self.alexa_counts * self.alexa_vc.transform([domain]).T\n",
        "        dict_match = self.dict_counts * self.dict_vc.transform([domain]).T\n",
        "        print(('%s Alexa match: %d, Dict match: %d' % (domain, alexa_match, dict_match)))\n",
        "        return domain, alexa_match, dict_match\n",
        "\n",
        "    def predict(self, domain):\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            _alexa_match = self.alexa_counts * \\\n",
        "                self.alexa_vc.transform(\n",
        "                    [domain]).T  # Matrix multiply and transpose\n",
        "            _dict_match = self.dict_counts * self.dict_vc.transform([domain]).T\n",
        "            _X = [len(domain), self.entropy(\n",
        "                domain), _alexa_match, _dict_match]\n",
        "            if int(sklearn.__version__.split('.')[1]) > 20:\n",
        "                _X = [_X]\n",
        "            return self.clf.predict(_X)[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rITmDweia80L"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/llSourcell/antivirus_demo/master/data.csv'\n",
        "\n",
        "data = pd.read_csv(url, sep='|')\n",
        "#X = data['ip'].values\n",
        "#y = data['url'].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RtXPMLW6Nhh"
      },
      "source": [
        "def levenshtein(source, target):\n",
        "    # Source\n",
        "    # https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
        "    if len(source) < len(target):\n",
        "        return levenshtein(target, source)\n",
        "\n",
        "    # So now we have len(source) >= len(target).\n",
        "    if len(target) == 0:\n",
        "        return len(source)\n",
        "\n",
        "    # We call tuple() to force strings to be used as sequences\n",
        "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
        "    source = np.array(tuple(source))\n",
        "    target = np.array(tuple(target))\n",
        "\n",
        "    # We use a dynamic programming algorithm, but with the\n",
        "    # added optimization that we only need the last two rows\n",
        "    # of the matrix.\n",
        "    previous_row = np.arange(target.size + 1)\n",
        "    for s in source:\n",
        "        # Insertion (target grows longer than source):\n",
        "        current_row = previous_row + 1\n",
        "\n",
        "        # Substitution or matching:\n",
        "        # Target and source items are aligned, and either\n",
        "        # are different (cost of 1), or are the same (cost of 0).\n",
        "        current_row[1:] = np.minimum(\n",
        "            current_row[1:],\n",
        "            np.add(previous_row[:-1], target != s))\n",
        "\n",
        "        # Deletion (target grows shorter than source):\n",
        "        current_row[1:] = np.minimum(\n",
        "            current_row[1:],\n",
        "            current_row[0:-1] + 1)\n",
        "\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvM8DhULIlDk",
        "outputId": "70ee6010-16a8-4342-a7f2-b880eb2c380c"
      },
      "source": [
        "dga_c = dga_classifier()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Initializing... training classifier - Please wait.\n",
            "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
            "                ngram_range=(3, 5), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "[+] Training classifier on training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
            "  warn(\"Some inputs do not have OOB scores. \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
            "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Out of sample legit f1 score 0.9988381234650359\n",
            "[+] Out of sample dga f1 score 0.6288888888888889\n",
            "[+] Training final classifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
            "  warn(\"Some inputs do not have OOB scores. \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
            "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Classifier Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZzSLOUv9Ryp",
        "outputId": "ebbe0ded-1670-4f06-e236-48e0e4f654f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Test scripts\n",
        "print(dga_c.predict('4dg6Fdfa'))\n",
        "levenshtein('google.com', 'gool.com')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dga\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}
