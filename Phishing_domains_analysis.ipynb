{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phishing domains analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbtoVcB7TETeq7Ip+1E6kR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paradoxxs/Paradoxxs.github.io/blob/main/Phishing_domains_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDB5Cnh7PSK0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGW2OZPFakDF"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import ensemble\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CDw0swKd5GH"
      },
      "source": [
        "class Alexa(object):\n",
        "\n",
        "    def __init__(self, limit=1000000):\n",
        "        self.limit = limit\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        return self.domain_in_alexa(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he_yrWuFKOv4"
      },
      "source": [
        "def entropy(s):\n",
        "    p, lns = Counter(s), float(len(s))\n",
        "    return -sum(count / lns * math.log(count / lns, 2) for count in list(p.values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lls-SxQ2bw5S"
      },
      "source": [
        "class dga_classifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        print('[*] Initializing... training classifier - Please wait.')\n",
        "        #self.a = Alexa()\n",
        "        dga_df = pd.read_csv('https://raw.githubusercontent.com/austin-taylor/flare/master/flare/data/misc/dga_domains.txt', names=['raw_domain'], header=None, encoding='utf-8')\n",
        "        word_df = pd.read_csv('https://raw.githubusercontent.com/austin-taylor/flare/master/flare/data/misc/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')\n",
        "        self.entropy = entropy\n",
        "        #self.domain_extract = domain_extract\n",
        "\n",
        "        alexa_df = pd.read_csv('https://raw.githubusercontent.com/austin-taylor/flare/master/flare/data/alexa/top-1m.csv')\n",
        "        alexa_df.columns = ['uri']\n",
        "\n",
        "        alexa_df['domain'] = 'empt'\n",
        "        alexa_df['domain'] =  [alexa_df['uri'][idx].split('.')[0].strip().lower() for idx in alexa_df.index ]\n",
        "\n",
        "        #alexa_df['domain'] = [self.domain_extract(uri) for uri in alexa_df['uri']]\n",
        "        alexa_df = alexa_df.dropna()\n",
        "        alexa_df = alexa_df.drop_duplicates()\n",
        "        alexa_df['class'] = 'legit'\n",
        "        alexa_df = alexa_df.reindex(np.random.permutation(alexa_df.index))\n",
        "        alexa_total = alexa_df.shape[0]\n",
        "        alexa_df = alexa_df[:int(alexa_total * .9)]\n",
        "        alexa_df[:10].index\n",
        "\n",
        "\n",
        "\n",
        "        # Blacklist values just differ by captilization or .com/.org/.info\n",
        "   \n",
        "\n",
        "        # Remove Top level domain\n",
        "        dga_df['domain'] = 'empt'\n",
        "        dga_df['domain'] =  [dga_df['raw_domain'][idx].split('.')[0].strip().lower() for idx in dga_df.index]\n",
        "       \n",
        "        #for idx in dga_df.index:\n",
        "         # dga_df['domain'][idx] = dga_df['raw_domain'][idx].split('.')[0].strip().lower() \n",
        "        #dga_df['domain'] = dga_df.applymap(lambda x: x.split('.')[0].strip().lower())\n",
        "      \n",
        "        # Cleanup any blank lines or dups\n",
        "        dga_df = dga_df.dropna()\n",
        "        dga_df = dga_df.drop_duplicates()\n",
        "        dga_total = dga_df.shape[0]\n",
        "\n",
        "        # Set Class\n",
        "        dga_df['class'] = 'dga'\n",
        "\n",
        "        # Hold out 10% of DGA\n",
        "        dga_df = dga_df[:int(dga_total * .9)]\n",
        "\n",
        "        # Merge Domains\n",
        "        all_domains = pd.concat([alexa_df, dga_df], ignore_index=True)\n",
        "\n",
        "        # Features\n",
        "        all_domains['length'] = [len(x) for x in all_domains['domain']]\n",
        "        all_domains = all_domains[all_domains['length'] > 6]\n",
        "        all_domains['entropy'] = [self.entropy(\n",
        "            x) for x in all_domains['domain']]\n",
        "\n",
        "\n",
        "        self.alexa_vc = feature_extraction.text.CountVectorizer(\n",
        "            analyzer='char', ngram_range=(3, 5), min_df=1e-4, max_df=1.0)\n",
        "        print(self.alexa_vc)\n",
        "        counts_matrix = self.alexa_vc.fit_transform(alexa_df['domain'])\n",
        "        self.alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
        "\n",
        "        # Read in word dictionary for trigrams\n",
        "        word_df = word_df[word_df['word'].map(lambda x: str(x).isalpha())]\n",
        "        word_df = word_df.applymap(lambda x: str(x).strip().lower())\n",
        "        word_df = word_df.dropna()\n",
        "        word_df = word_df.drop_duplicates()\n",
        "\n",
        "        self.dict_vc = feature_extraction.text.CountVectorizer(\n",
        "            analyzer='char', ngram_range=(3, 5), min_df=1e-5, max_df=1.0)\n",
        "        counts_matrix = self.dict_vc.fit_transform(word_df['word'])\n",
        "        self.dict_counts = np.log10(counts_matrix.sum(axis=0).getA1())\n",
        "\n",
        "        all_domains['alexa_grams'] = self.alexa_counts * \\\n",
        "            self.alexa_vc.transform(all_domains['domain']).T\n",
        "        all_domains['word_grams'] = self.dict_counts * \\\n",
        "            self.dict_vc.transform(all_domains['domain']).T\n",
        "        all_domains['diff'] = all_domains[\n",
        "            'alexa_grams'] - all_domains['word_grams']\n",
        "\n",
        "        weird_cond = (all_domains['class'] == 'legit') & (\n",
        "            all_domains['word_grams'] < 3) & (all_domains['alexa_grams'] < 2)\n",
        "\n",
        "\n",
        "        not_weird = all_domains[all_domains['class'] != 'weird']\n",
        "        X = not_weird[['length', 'entropy', 'alexa_grams', 'word_grams']].to_numpy()\n",
        "\n",
        "        # Labels (scikit learn uses 'y' for classification labels)\n",
        "        y = np.array(not_weird['class'].tolist())\n",
        "\n",
        "        # Train on a 80/20 split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2)\n",
        "\n",
        "        print('[+] Training classifier on training set')\n",
        "\n",
        "        clf = ensemble.RandomForestClassifier(n_estimators=20, oob_score=True)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        print('[+] Out of sample legit f1 score {}'.format(f1_score(\n",
        "            y_test == 'legit', \n",
        "            y_pred == 'legit',\n",
        "            pos_label = 1,\n",
        "        )))\n",
        "\n",
        "        print('[+] Out of sample dga f1 score {}'.format(f1_score(\n",
        "            y_test == 'legit', \n",
        "            y_pred == 'legit',\n",
        "            pos_label = 0,\n",
        "        )))\n",
        "        \n",
        "        print('[+] Training final classifier')\n",
        "        self.clf = ensemble.RandomForestClassifier(n_estimators=20, oob_score=True)\n",
        "        self.clf.fit(X, y)\n",
        "        print('[+] Classifier Ready')\n",
        "\n",
        "    def ngram_count(self, domain):\n",
        "        # Multiply and transpose vector\n",
        "        alexa_match = self.alexa_counts * self.alexa_vc.transform([domain]).T\n",
        "        dict_match = self.dict_counts * self.dict_vc.transform([domain]).T\n",
        "        print(('%s Alexa match: %d, Dict match: %d' % (domain, alexa_match, dict_match)))\n",
        "        return domain, alexa_match, dict_match\n",
        "\n",
        "    def predict(self, domain):\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            _alexa_match = self.alexa_counts * \\\n",
        "                self.alexa_vc.transform(\n",
        "                    [domain]).T  # Matrix multiply and transpose\n",
        "            _dict_match = self.dict_counts * self.dict_vc.transform([domain]).T\n",
        "            _X = [len(domain), self.entropy(\n",
        "                domain), _alexa_match, _dict_match]\n",
        "            if int(sklearn.__version__.split('.')[1]) > 20:\n",
        "                _X = [_X]\n",
        "            return self.clf.predict(_X)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rITmDweia80L"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/llSourcell/antivirus_demo/master/data.csv'\n",
        "\n",
        "data = pd.read_csv(url, sep='|')\n",
        "#X = data['ip'].values\n",
        "#y = data['url'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RtXPMLW6Nhh"
      },
      "source": [
        "def levenshtein(source, target):\n",
        "    # Source\n",
        "    # https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
        "    if len(source) < len(target):\n",
        "        return levenshtein(target, source)\n",
        "\n",
        "    # So now we have len(source) >= len(target).\n",
        "    if len(target) == 0:\n",
        "        return len(source)\n",
        "\n",
        "    # We call tuple() to force strings to be used as sequences\n",
        "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
        "    source = np.array(tuple(source))\n",
        "    target = np.array(tuple(target))\n",
        "\n",
        "    # We use a dynamic programming algorithm, but with the\n",
        "    # added optimization that we only need the last two rows\n",
        "    # of the matrix.\n",
        "    previous_row = np.arange(target.size + 1)\n",
        "    for s in source:\n",
        "        # Insertion (target grows longer than source):\n",
        "        current_row = previous_row + 1\n",
        "\n",
        "        # Substitution or matching:\n",
        "        # Target and source items are aligned, and either\n",
        "        # are different (cost of 1), or are the same (cost of 0).\n",
        "        current_row[1:] = np.minimum(\n",
        "            current_row[1:],\n",
        "            np.add(previous_row[:-1], target != s))\n",
        "\n",
        "        # Deletion (target grows shorter than source):\n",
        "        current_row[1:] = np.minimum(\n",
        "            current_row[1:],\n",
        "            current_row[0:-1] + 1)\n",
        "\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gvM8DhULIlDk",
        "outputId": "ea044faf-119d-40bc-d7a5-92e5702cf3d3"
      },
      "source": [
        "dga_c = dga_classifier()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Initializing... training classifier - Please wait.\n",
            "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
            "                ngram_range=(3, 5), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "[+] Training classifier on training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
            "  warn(\"Some inputs do not have OOB scores. \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
            "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Out of sample legit f1 score 0.9989533602005654\n",
            "[+] Out of sample dga f1 score 0.6495925494761351\n",
            "[+] Training final classifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
            "  warn(\"Some inputs do not have OOB scores. \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
            "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Classifier Ready\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a40d8ce5abe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdga_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdga_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-e9b034d9063e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0;32m-> 1552\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m                 ctypes.byref(self.handle)))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m    801\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot initialize Dataset from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label should not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mset_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_to_1d_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_label_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mlist_to_1d_numpy\u001b[0;34m(data, dtype, name)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_1d_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'legit'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZzSLOUv9Ryp"
      },
      "source": [
        "#domain_df = df.read_csv('')\n",
        "#domain_df.columns['domain']\n",
        "#domain_df.drop_duplicates()\n",
        "#domain_df[score] = [dga_c.predict(['domain'][idx]) for idx in domain_df.index]\n",
        "\n",
        "#calucating the levenshtein score of all the domain user have connect to compared to the top 1000 domain in the alexa score. \n",
        "#domain_df[score] = [levenshtein(['domain'][idx]) for idx in domain_df.index]\n",
        "#for idx in alexa_df[:1000].index:\n",
        "  #for idx2 in domain_df.index\n",
        "    #score = levenshtein(alexa_df['domain'][idx],domain_df['domain'][idx2])\n",
        "    #if(score < 2): \n",
        "      #print(domain_df['domain'][idx2] + ' ' + score)\n",
        "\n",
        "\n",
        "#Test scripts\n",
        "#print(dga_c.predict('2323eafsa'))\n",
        "#levenshtein('google.com', 'gool.com')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}